### 1. Définitions

Pour optimiser le temps d’exécution d'un programme il est souvent pertinent d'avoir recours à la programmation concurrente ou parallèle plutôt que séquentielle. Au sein de ce notebook, nous allons passer en revue ces concepts et proposer quelques implémentations sur Python.

#### Eléments essentiels 

 - Un **processeur** (ou CPU pour Central Processing Unit), est l’un des composants électroniques essentiels que l’on trouve dans nos ordinateurs et qui se charge de l’**exécution d’instructions** qui lui sont données. Un processeur est principalement défini par 2 caractéristiques : Sa **fréquence** et son **nombre de cœurs**. La fréquence associée au processeur correspond au **nombre de cycles** qu’il est capable de réaliser par seconde. Un processeur peut avoir **un ou plusieurs cœurs** qui correspondent à des **unités de calcul**. Un processeur **multicoeur** pourra exécuter plusieurs tâches simultanément, si ces tâches le permettent, en répartissant les tâches par cœur disponible.

 - La **RAM** pour Random Access Memory correspond à la mémoire vive d'un ordinateur. C'est un espace de stockage temporaire. Le système accède à cette mémoire de façon instantanée ce qui permet la fluidité de l'interface.

#### Processus et thread 

On appelle **processus** un programme autrement dit un ensemble d'instructions qui est **en cours d'exécution**. L'ensemble des instructions d'un programme est stocké sur la **RAM** de telle sorte à ce que le processeur puisse comprendre ces instructions. A partir de la RAM, le processeur va donc exécuter chaque instruction. On parle de cycle "_fetch-execute_", cela signie que le processeur cherche l'instruction puis l'exécute. Il est important de noter que chaque processus est indépendant des autres existants.

Un **thread** est une sous-partie de processus et correspond à un **fil d'exécution** d'instructions. Ainsi un processus pourra être composé d'un ou plusieurs threads, qui seront exécutés de façon parallèle ou concurrente.

Processus et threads sont ainsi des notions très proches mais elles diffèrent sur des éléments clés. Chaque **processus** dispose d'un **espace mémoire propre** qui lui est alloué. A contrario, tous les threads appartenant à un même processus **partagent un espace mémoire**. Ces propriétés concernant l'allocation de la mémoire influent notamment sur la communication et la rapidité de passage d'un processus ou thread à un autre. En effet, la communication entre **threads** sera par exemple **moins coûteuse** qu'une communication entre processus et il sera également plus rapide de passer d'un thread à un autre plutôt qu'un processus à un autre. Il est également à noter que la création d'un thread sera moins coûteuse en temps que la création d'un processus à cause de l'allocation de cet espace mémoire propre.

#### Exécution synchrone et asynchrone 
On parle d'exécution **synchrone** lorsqu'un programme est exécuté en **séquentiel**. Si l'on considère un fil d'instructions, alors l'exécution de chaque instruction est conditionnée à la fin de l'exécution de l'instruction précédente. Il est important de noter que les instructions portent ici un ordre. L'illustration ci-dessous illustre le fonctionnement en séquentiel.

![](https://assets-datascientest.s3-eu-west-1.amazonaws.com/python_avance/mpmt_im1.png)

Dans cet exemple, le processus 2 attend la complétion du processus 1 pour démarrer. De même le processus 3 attend la complétion du processus 2 pour démarrer.

L'exécution **asynchrone** correspond au fait d'exécuter des nouvelles instructions alors que les instructions précédentes n'ont pas encore fini leur exécution : l'exécution d'une nouvelle instruction ne sera donc plus conditionnée à la complétion de l'instruction précédente. L'illustration ci-dessous illustre le fonctionnement en asynchrone.

![](https://assets-datascientest.s3-eu-west-1.amazonaws.com/python_avance/mpmt_im2.png)

Dans cet exemple, l'ordre de complétion des processus n'est pas le même que l'ordre dans lequel ils ont été créés car leur exécution est faite de manière asynchrone.

#### Programme I/O bound et CPU bound

##### I/O bound 

On parle de programme **I/O bound** (limité par les entrées et sorties) lorsqu'il comporte des opérations d'entrées et de sorties qui déterminent principalement son temps d'exécution. Un programme qui comporte de nombreuses opérations de **lecture et d'écriture** d'informations externes (c'est-à-dire qui ne sont pas stockées au sein la RAM), telles que des saisies utilisateurs ou encore des requêtes adressées à une base de données, pourra être considéré comme I/O bound. Ces opérations vont donner lieu à de l'**attente**, souvent appelé IOwait, car ces données ne sont pas immédiatement accessible par le CPU. L'exécution du reste des instructions va donc être mise en pause en attendant l'obtention de ces informations.

La programmation **asynchrone** vise à combler ces temps d'attente. A titre d'exemple, lorsque l'on attend une réponse suite à une requête (opération I/O), si le programme est asynchrone, il est possible de lancer d'autres requêtes alors que le résultat de la première est toujours en attente. Il faut néanmoins rester vigilant car le "_multitasking_" permis par la programmation asynchrone peut parfois faire diminuer les performances car il peut s'avérer coûteux en temps. Pour désigner le passage d'une tâche à une autre, on fera souvent référence à la notion de "_context-switching_" : passser d'un thread ou d'un processus à un autre.

##### CPU bound

On dit qu'un ensemble d'instructions est CPU bound lorsque le temps d'exécution global est limité par les performances du CPU. Cela est notamment le cas d'un programme qui comporte de nombreux calculs matriciels. Ainsi plus le processeur sera performant, plus l'exécution du programme sera rapide.

#### Concurrence et parallélisme 

Les notions de **concurrence** et **parallélisme** sont souvent confondues car très proches. Il est important de les distinguer. Pour cela il faut différencier l'exécution **concurrente** et **parallèle**.

##### Parallélisme 

On parle d'exécution **parallèle** lorsque des tâches sont exécutées et progressent **simultanément** et **indépendamment**. Il peut s'agir de threads ou de processus.

##### Concurrence

On parle d'exécution **concurrente** lorsque plusieurs tâches progressent dans un même intervalle de temps mais s'exécutent en différé. Ce délai est bien souvent infime ce qui donne l'illusion d'une exécution simultanée. Rappelons néanmoins qu'un coeur de CPU ne peut exécuter qu'une tâche à la fois et que c'est l'exploitation des temps de latence qui donnent l'illusion de simultanéité. A titre d'exemple, une seconde tâche peut être exécutée suite à l'exécution d'une première qui est toujours en cours de complétion.

###### Quoi choisir entre threads et processus ? 

Il n'existe pas de règle générale pour faire un choix entre threads et processus, néanmoins souvent il sera pertinent de choisir :

- Plusieurs threads et une programmation concurrente lorsque le programme est I/O bound;
    
- Plusieurs processus et une programmation parallèle lorsque le programme est CPU bound.
    

#### Multithreading et multiprocessing

Lorsque l'on fait référence au **multiprocessing**, on désigne souvent l'exécution de **plusieurs processus en simultané** tandis que pour le **multithreading** on fait référence à l'exécution de **plusieurs threads en concurrence**.

##### Multithreading 

Lorsque l'on fait référence au **multithreading**, on désigne dans la majorité des cas l'exécution de plusieurs threads au sein d'un même coeur. C'est donc de la programmation concurrente, qui permet entre autres d'exploiter les temps de latence. Il est important de noter que dans ce cas il ne s'agit pas d'exécution simultanée.

Lorsque que le processeur dispose de plusieurs coeurs alors les threads peuvent être répartis entre les coeurs et peuvent donc être exécutés simultanément, c'est ici alors de la programmation parallèle.

Il est important de souligner que le partage de mémoire entre threads et leur exécution en parallèle est à l'origine de certaines sécurités telles que les "_locks_" ou "_synchronizers_" qui permettent d'éviter autant faire que ce peut des erreurs. A titre d'exemple, si l'on incrémente un même nombre entre deux threads qui sont exécutés en parallèle alors il est possible de "_perdre_" ces incrémentations car les threads ont été exécutés simultanément. Les "_synchronisations_" entre threads ainsi que des "_verrous_" permettent en règle générale d'ôter ou au moins de limiter ces erreurs.

###### Le cas Python 

Python est un cas particulier en ce qui concerne le multithreading. En effet, il possède un verrou appelé GIL pour Global Interpreter Lock en anglais qui assure qu'un seul thread n'ait accès à l'interpréteur Python a la fois. Cela rejoint ainsi la gestion des erreurs d'accessibilité de mémoire évoquée plus haut.

##### Multiprocessing 

Lorsque l'on fait référence au **multiprocessing**, on désigne l'exécution des processus en simultané. C'est ce que l'on appelle la programmation parallèle. L'exécution des processus est ainsi répartie entre les coeurs du processeur qui vont donc être exécutés séparément et simultanément.

